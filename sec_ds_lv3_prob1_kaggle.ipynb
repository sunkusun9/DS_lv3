{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765c0422",
   "metadata": {},
   "source": [
    "# 문제 6\n",
    "\n",
    "[Kaggle 형] train_prob.csv로 문제 failure 예측하는 모델을 만들고, \n",
    "\n",
    "test_prob.csv에 대한 failure가 1일 확률 예측하여 다음과 같은 형식의 answer6.csv를 만들어라. \n",
    "\n",
    "측정 지표는 AUC(area under of ROC curve)이다. id 는 테스트 케이스의 id 이고, failure에는 failure가 1이 될 확률이다.\n",
    "\n",
    "id,failure\n",
    "\n",
    "16115, 0.1\n",
    "\n",
    "16116, 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ab065",
   "metadata": {},
   "source": [
    "# Kaggle Tip\n",
    "\n",
    "**Pain point** \n",
    "\n",
    "> 데이터처리 과정을 Train에서 학습하고 변환 결과를 Train과 Test 데이터프레임에 적용하거나 생성\n",
    "\n",
    "> 여러 처리기를 변수별로 따로 적용\n",
    "\n",
    "**데이터처리 과정와 모델 학습을 한 번에**\n",
    "\n",
    "=> sklearn.compsose.ColumnTransformer + sklearn.pipeline.make_pipeline\n",
    "\n",
    "> ColumnTransformer : DataFrame에 컬럼별 처리기(StandardScaler, OneHotEncoder, PCA, ...) 적용\n",
    "\n",
    "> make_pipeline: 단계화된 처리 모델 구성\n",
    "\n",
    "\n",
    "Ex) \n",
    "\n",
    "cat0, cat1는 가변수, cont0, cont1: 표준화 , na_1, na_2: 통과\n",
    "\n",
    "위 처리를 거친후 대상 변수 failure를  로직스틱 회귀 모델로 학습 \n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skelarn.pipeline import make_pipeline\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(), ['cat0', 'cat1']), #  cat0, cat1은 OneHotEncoder를 적용\n",
    "    ('std', StandardScaler(), ['cont0', 'cont1']), #  cont0, cont1은 StandardScaler를 적용\n",
    "    ('pt', 'passthrough', ['na_1', 'na_2']) # na_1, na_2는 통과 \n",
    "], remainder='drop') # remainder: 'drop' 나머지는 제외, 'passthrough'는 나머지 통과\n",
    "X = ['cat0', 'cat1', 'cont0', 'cont1', 'na_1', 'na_2']\n",
    "clf_lr = make_pipeline(ct, LogisticRegression(solver='lbfgs')) # ColumnTransformer->LogisticRegression pipeline 구성\n",
    "clf_lr.fit(df[X], df['failure'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1edb86ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faeb7b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_prob.csv', index_col='id')\n",
    "df_test = pd.read_csv('test_prob.csv', index_col='id')\n",
    "df_ans = pd.read_csv('test_prob_ans.csv', index_col='id') # 실제 시험에서는 주어지지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d061f25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    5765\n",
       "E    5343\n",
       "B    5250\n",
       "A    5100\n",
       "Name: product_code, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['product_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49ab2fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D    5112\n",
       "Name: product_code, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['product_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a864008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "imp = IterativeImputer(\n",
    "    estimator=LinearRegression(),\n",
    "    random_state=123\n",
    ")\n",
    "X_imp = ['measurement_{}'.format(i) for i in range(3, 10)] + ['measurement_17']\n",
    "\n",
    "df_train = df_train.assign(\n",
    "    na_1=lambda x: x['measurement_3'].isna(),\n",
    "    na_2=lambda x: x['measurement_5'].isna(),\n",
    ")\n",
    "\n",
    "df_test = df_test.assign(\n",
    "    na_1=lambda x: x['measurement_3'].isna(),\n",
    "    na_2=lambda x: x['measurement_5'].isna(),\n",
    ")\n",
    "\n",
    "# Train에 등장했던 product_code 범주값이 테스트에는 등장하지 않아 각각 따로 모델을 구성(fit_transform)을 하게 됩니다.\n",
    "# 일반적으로는 train에 했던 내용을 test 그대로 적용하지만 데이터의 특성상 이렇게 처리하는 것이니,\n",
    "# 꼭 염두해주세요.\n",
    "df_train[X_imp] = df_train.groupby('product_code')[X_imp].apply(\n",
    "    lambda x: pd.DataFrame(imp.fit_transform(x), index=x.index, columns=X_imp)\n",
    ")\n",
    "df_test[X_imp] = df_test.groupby('product_code')[X_imp].apply(\n",
    "    lambda x: pd.DataFrame(imp.fit_transform(x), index=x.index, columns=X_imp)\n",
    ")\n",
    "\n",
    "na_mean_cols = ['measurement_{}'.format(i) for i in range(10, 17)]\n",
    "df_train[na_mean_cols] = df_train.groupby('product_code')[na_mean_cols].transform(lambda x: x.fillna(x.mean()))\n",
    "df_test[na_mean_cols] = df_test.groupby('product_code')[na_mean_cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df_train['loading'] = df_train['loading'].fillna(df_train['loading'].mean())\n",
    "# loading은 product_code로 구분되는 것은 아니니, train을 사용합니다.\n",
    "df_test['loading'] = df_test['loading'].fillna(df_train['loading'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7585014d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('std',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  ['loading', 'measurement_1',\n",
       "                                                   'measurement_4',\n",
       "                                                   'measurement_14',\n",
       "                                                   'measurement_17']),\n",
       "                                                 ('pt', 'passthrough',\n",
       "                                                  ['na_1'])],\n",
       "                                   verbose=False)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 3의 내용을 뽑아와서 베이스라인 모델을 만들어 냅니다.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 문제 3의 속성(입력변수) 선택의 최종결과만 취합니다.\n",
    "X_model_1 = ['loading', 'measurement_1', 'measurement_4', 'measurement_14', 'measurement_17', 'na_1']\n",
    "\n",
    "ct_model_1 = ColumnTransformer([\n",
    "    ('std', StandardScaler(), X_model_1[:-1]),\n",
    "    ('pt', 'passthrough', ['na_1'])\n",
    "])\n",
    "clf_lr = make_pipeline(ct_model_1, LogisticRegression(solver='lbfgs'))\n",
    "clf_lr.fit(df_train[X_model_1], df_train['failure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bccaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer6.csv에 test 예측 결과를 출력합니다.\n",
    "pd.DataFrame({'target': clf_lr.predict_proba(df_test[X_model_1])[:, 1]}, index=df_test.index)\\\n",
    "    .to_csv('answer6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1169cd61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5883988309352517"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자가 체점을 해봅니다.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(df_ans['failure'], clf_lr.predict_proba(df_test[X_model_1])[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6532610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
